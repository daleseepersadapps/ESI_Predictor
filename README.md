# Triage Needs No Black Box: Distilling LLMs for Transparent Emergency Severity Index Prediction

Traditional machine learning methods for Emergency Severity Index (ESI) prediction often lack interpretability, relying on complex statistical correlations or feature importance rankings that are not easily reviewed in a clinical setting [1]. To safely integrate machine-generated medical decisions into practice, human-readable reasoning—such as step-by-step explanations of predictions—is essential for clinical trust and verification. Interpretable models are essential. This study compares large language models (LLMs) to nurse-assigned ESI levels and introduces a novel, lightweight model that integrates reasoning outputs and contextual learning to enhance accuracy and interpretability.

We utilize a dataset from US Acute Care Solutions (USACS), comprising clinical data from emergency department visits across multiple hospitals. A validated reference standard for ESI levels was established using resource utilization, patient disposition, and mortality, aligned with ESI Guidelines. Nurse-assigned ESI levels were compared to the reference standard for baseline performance. A zero-shot LLM approach was applied to predict ESI levels from unstructured triage narratives. We then employ a knowledge distillation approach whereby a large ‘teacher’ model (DeepSeek-R1) generates chain-of-thought reasoning, which is then used to supervise the fine-tuning of a smaller ‘student’ model (Qwen2-1.5b).

Preliminary findings indicate that the distilled LLM not only surpasses the zero-shot approach and nurse-assigned ESI levels in prediction accuracy but also enhances interpretability through structured reasoning outputs. Benchmarking LLMs against clinical practice highlights their potential to improve triage efficiency and consistency. By enabling these models to run on affordable hardware, such as a Raspberry Pi 5, they can be widely adopted, ensuring equitable access to medical AI solutions.

